#!/usr/bin/env ruby
#
# Copyright 2008-2009 Amazon.com, Inc. or its affiliates.  All Rights Reserved.

$LOAD_PATH << File.dirname(__FILE__)

require 'amazon/coral/elasticmapreduceclient'
require 'amazon/retry_delegator'
require 'optparse'
require 'set'
require 'json'

AMAZON_ELASTIC_MAP_REDUCE_CLIENT_VERSION = '$Date: 2010/06/02 $'
VALID_METHODS = Set.new %w(DescribeJobFlows TerminateJobFlows RunJobFlow AddJobFlowSteps)

# This is the current default Hadoop version
DEFAULT_HADOOP_VERSION = "0.20"

class Hash
  alias :/ :[]
end

class Array
  alias :/ :[]
end

class NilClass
  def /(key)
    nil
  end
end

class Set
  def join(*args)
    self.to_a.join(*args)
  end
end

def argument_error_if_nil(value, message)
  if value == nil then
    raise ArgumentError, message
  end
end

def trace_exception(options, e)
  if options[:debug] then 
    puts e.backtrace.join("\n")
  end
end

def is_error_response(response)
  response.key?('Error')
end

is_retryable_error_response = Proc.new do |response| 
  if response == nil then
    false
  else
    ret = false
    if response['Error'] then 
      # note: 'Timeout' is not retryable because the operation might have completed just the connection timed out
      ret ||= ['InternalFailure', 'Throttling', 'ServiceUnavailable'].include?(response['Error']['Code'])
    end
    ret 
  end
end

CLOSED_DOWN_STATES        = Set.new(%w(TERMINATED SHUTTING_DOWN COMPLETED FAILED))
WAITING_OR_RUNNING_STATES = Set.new(%w(WAITING RUNNING))

def jobflow_state(job_description)
  job_description['ExecutionStatusDetail']['State']
end

def is_closed_down(job_description)
  return CLOSED_DOWN_STATES.include?(jobflow_state(job_description))
end

def is_waiting_or_running(job_description)
  return WAITING_OR_RUNNING_STATES.include?(jobflow_state(job_description))
end

GENERIC_OPTIONS = Set.new(%w(-conf -D -fs -jt -files -libjars -archives))

def sort_streaming_args(streaming_args)
  sorted_streaming_args = []
  i=0
  while streaming_args && i < streaming_args.length
    if GENERIC_OPTIONS.include?(streaming_args[i]) then
      if i+1 < streaming_args.length
        assert_not_matching(streaming_args[i], streaming_args[i+1], /^-/)
        sorted_streaming_args.unshift(streaming_args[i+1])
        sorted_streaming_args.unshift(streaming_args[i])
        i=i+2
      else
        raise RuntimeError, "Missing value for argument #{streaming_args[i]}"
      end
    else
      sorted_streaming_args << streaming_args[i]
      i=i+1
    end
  end
  return sorted_streaming_args
end

def parse_credentials(credentials, options)
  conversions = [
    # These first ones use an incorrect naming convenion, but we keep them around for
    # backwards compatibility
    [:aws_access_id, "access_id"],
    [:aws_secret_key, "private_key"],
    [:key_pair, "keypair"], 
    [:key_pair_file, "keypair-file"], 
    [:log_uri, "log_uri"], 
    
    # Now the current ones
    [:aws_access_id, "access-id"],
    [:aws_secret_key, "private-key"],
    [:key_pair, "key-pair"], 
    [:key_pair_file, "key-pair-file"], 
    [:log_uri, "log-uri"], 
    [:endpoint, "endpoint"], 
    [:region, "region"], 
    [:enable_debugging, "enable-debugging"],
    [:hadoop_version, "hadoop-version"],
  ]

  env_options = [
    ['ELASTIC_MAPREDUCE_ACCESS_ID',         :aws_access_id],
    ['ELASTIC_MAPREDUCE_PRIVATE_KEY',       :aws_secret_key],
    ['ELASTIC_MAPREDUCE_KEY_PAIR',          :key_pair],
    ['ELASTIC_MAPREDUCE_KEY_PAIR_FILE',     :key_pair_file],
    ['ELASTIC_MAPREDUCE_LOG_URI',           :log_uri],
    ['ELASTIC_MAPREDUCE_APPS_PATH',         :apps_path],
    ['ELASTIC_MAPREDUCE_BETA_PATH',         :beta_path],
    ['ELASTIC_MAPREDUCE_ENDPOINT',          :endpoint],
    ['ELASTIC_MAPREDUCE_REGION',            :region],
    ['ELASTIC_MAPREDUCE_HADOOP_VERSION',    :hadoop_version],
    ['ELASTIC_MAPREDUCE_ENABLE_DEBUGGING',  :enable_debugging]
  ]

  for env_key, option_key in env_options do
    if ! options[option_key] && ENV[env_key] then
      options[option_key] = ENV[env_key]
    end
  end

  candidates = [
    credentials, 
    ENV['ELASTIC_MAPREDUCE_CREDENTIALS'], 
    File.join(File.dirname(__FILE__), credentials),
    File.join(ENV['HOME'],  "." + credentials), 
    File.join(ENV['HOME'],  credentials)
  ]
  filename   = candidates.find { |fname| File.exist?(fname) if fname }
  if filename == nil then
    raise RuntimeError, "Couldn't find a credentials file after searching for: #{candidates.join(", ")}"
  end
  
  begin
    credentials_hash = JSON.parse(File.read(filename))
    for option_key, credentials_key in conversions do
      if credentials_hash[credentials_key] && !options[option_key] then
        options[option_key] = credentials_hash[credentials_key]
      end
    end
  rescue Exception => e
    raise ArgumentError, "Unable to parse #{filename}: #{e.message}"
  end
  
  argument_error_if_nil(options[:aws_access_id], "Missing key access-id")
  argument_error_if_nil(options[:aws_secret_key], "Missing key private-key")

end

def assert_not_matching(name, value, *patterns)
  for pat in patterns do
    if value.match(pat) then
      raise RuntimeError, "Invalid argument for #{name}: '#{value}' is not a valid argument"
    end
  end
end

def assert_matching(name, value, *patterns)
  for pat in patterns do
    if value.match(pat) == nil then
      raise RuntimeError, "Invalid argument for #{name}: '#{value}' is not a valid argument"
    end
  end
end

def check_for_error(message)
  result = yield
  if is_error_response(result) then
    raise RuntimeError, (message + JSON.pretty_generate(result))
  end
  return result
end


options = {
  :credentials => 'credentials.json'
}

begin
  opts = OptionParser.new do |opts|

    ##
    ## Helper methods
    ##
    set_opt_flag = proc do | name, description, symbol | 
      opts.on(name, description) do |create|
        options[symbol] = true
      end
    end

    set_opt_value = proc do | name, description, symbol |
      opts.on(name, description) do |val|
        assert_not_matching(name.split[0], val, /^--/)
        options[symbol] = val
      end
    end

    set_opt_value_with_short = proc do | shortname, name, description, symbol |
      opts.on(shortname, name, description) do |val|
        assert_not_matching(name.split[0], val, /^--/)
        options[symbol] = val
      end
    end

    set_opt_value_matching = proc do | name, description, symbol, match_criteria |
      opts.on(name, description) do |val|
        assert_not_matching(name.split[0], val, /^--/)
        assert_matching(name.split[0], val, match_criteria)
        options[symbol] = val
      end
    end

    check_step_set = proc do | name, allowed_types |
      allowed_with_str = allowed_types.map { |v| "--" + v }.join(" ")
      if ! options[:steps] || ! options[:steps].last || ! allowed_types.include?(options[:steps].last[:type]) then
        raise ArgumentError, name.split[0] + " must follow one of " + allowed_with_str
      end
    end

    set_step_opt_value = proc do | name, description, symbol, allowed_types |
      opts.on(name, description) do |val|
        assert_not_matching(name.split[0], val, /^--/)
        check_step_set.call(name, allowed_types)
        
        options[:steps].last[symbol] = val
      end
    end

    set_step_opt_append = proc do | name, description, symbol, allowed_types |
      opts.on(name, description) do |val|
        assert_not_matching(name.split[0], val, /^--/)
        check_step_set.call(name, allowed_types)
        options[:steps].last[symbol] ||= []
        options[:steps].last[symbol] << val
      end
    end

    last_type_added = nil    

    push_step = proc do |args|
      last_type_added = :step
      options[:steps] ||= []
      options[:steps] << args
    end

    opts.separator "\n  Creating Job Flows\n"

    set_opt_flag.call( "--create", "Create a new job flow", :create)
    set_opt_value.call("--name NAME", "Name of the job flow", :name)
    set_opt_flag.call("--alive", "Create a job flow that stays running even though it has executed all its steps", :alive)
    set_opt_value_matching.call("--num-instances NUM", "Number of instances in the job flow", :num_instances, /^[0-9]+$/)
    set_opt_value.call("--instance-type TYPE", "The type of the instances to launch", :instance_type)
    set_opt_value.call("--slave-instance-type TYPE", "The type of the slave instances to launch", :slave_instance_type)
    set_opt_value.call("--master-instance-type TYPE", "The type of the master instance to launch", :master_instance_type)
    set_opt_value.call("--key-pair KEY_PAIR", "The name of your Amazon EC2 Keypair", :key_pair) 
    set_opt_value.call("--key-pair-file FILE_PATH", "Path to your local pem file for your EC2 key pair", :key_pair_file) 
    set_opt_value.call("--log-uri LOG_URI", "Location in S3 to store logs from the job flow, e.g. s3n://mybucket/logs", :log_uri)
    set_opt_value.call("--availability-zone A_Z", "Specify the Availability Zone in which to launch the jobflow", :az)
    set_opt_value.call("--info INFO", "Specify additional info in JSON", :ainfo)
    set_opt_value.call("--hadoop-version INFO", "Specify the Hadoop Version to install", :hadoop_version)
    set_opt_flag.call("--plain-output", "Return the job flow id from create step as simple text", :plain_output)

    opts.separator "\n  Adding Jar Steps to Job Flows\n"

    opts.on("--jar JAR", "Add a step that executes a jar") do |jar|
      assert_not_matching("--jar", jar, /^--/)
      push_step.call(:type => 'jar', :jar => jar)
    end
    
    opts.on("--wait-for-step", "Wait for the step to finish") do
      options[:wait_for_step] = true;
    end

    set_step_opt_value.call("--main-class MAIN_CLASS", "Specify main class for the JAR", :main_class, ['jar'])

    opts.separator "\n  Adding Streaming Steps to Job Flows\n"

    opts.on("--stream", "Add a step that performs hadoop streaming") do
      push_step.call(:type => 'streaming')
    end

    set_step_opt_value.call( "--input INPUT", "Input to the steps, e.g. s3n://mybucket/input", :input, ['streaming'])
    set_step_opt_value.call( "--output OUTPUT", "The output to the steps, e.g. s3n://mybucket/output", :output, ['streaming'])
    set_step_opt_value.call( "--mapper MAPPER", "The mapper program or class", :mapper, ['streaming'])
    set_step_opt_append.call("--cache CACHE_FILE", "A file to load into the cache, e.g. s3n://mybucket/sample.py#sample.py", :cache, ['streaming'])
    set_step_opt_append.call("--cache-archive CACHE_FILE", "A file to unpack into the cache, e.g. s3n://mybucket/sample.jar", :cache_archive, ['streaming'])
    set_step_opt_append.call("--jobconf KEY=VALUE", "Specify jobconf arguments to pass to streaming, e.g. mapred.task.timeout=800000", :jobconf, ['streaming'])
    set_step_opt_value.call( "--reducer REDUCER", "The reducer program or class", :reducer, ['streaming'])

    opts.separator "\n  Job Flow Deugging Options\n"

    opts.on("--enable-debugging", "Enable job flow debugging (you must be signed up to SimpleDB for this to work)") do
      push_step.call(:type => 'install-jobflow-debugging')
      options[:enable_debugging] = true
    end

    opts.separator "\n  Adding Pig steps to job flows\n"

    opts.on("--pig-script", "Add a step that runs a Pig script") do
      push_step.call(:type => 'pig-script')
    end


    opts.on("--pig-interactive", "Add a step that sets up the job flow for an interactive (via SSH) pig session") do
      push_step.call(:type => 'pig-interactive')
    end

    opts.separator "\n  Configuring a Hive on a JobFlow\n"

    opts.on("--hive-site HIVE_SITE", "Override Hive configuration with configuration from HIVE_SITE") do |site|
      if ! site.match(/^s3n?:\/\//) then
        raise RuntimeError, "Expected HIVE_SITE to be an s3 url, e.g. s3://mybucket/conf/mysite.xml"
      end
      push_step.call(:type => 'hive-site', :site => site)
    end

    opts.on("--hive-script", "Add a step that runs a Hive script") do
      push_step.call(:type => 'hive-script')
    end

    opts.on("--hive-interactive", "Add a step that sets up the job flow for an interactive (via SSH) hive session") do
      push_step.call(:type => 'hive-interactive')
    end

    opts.separator "\n  Adding Steps from a Json File to Job Flows\n"

    opts.on("--json FILE", "Add a sequence of steps stored in a json file") do |file|
      push_step.call(:type => 'json', :file => file)
    end

    opts.on("--param VARIABLE=VALUE", "subsitute <variable> with value in the json file") do |variable|
      assert_not_matching("--variable", variable, /^--/)
      if match = variable.match(/([^=]+)=(.*)/) then
        if ! options[:steps] || ! options[:steps].last then
          raise ArgumentError, "--param must follow either --jar or --stream"
        end
        options[:steps].last[:variables] ||= []
        options[:steps].last[:variables] << [match[1], match[2]]
      else
        raise RuntimeError, "Expected '#{variable}' to be in the form VARIABLE=VALUE"
      end
    end

    opts.separator "\n  Contacting the Master Node\n"

    opts.on("--no-wait", "Don't wait for the Master node to start before executing scp or ssh") do
      options[:wait] = false
    end
    
    opts.on("--ssh [COMMAND]", String, "SSH to the master node and optionally run a command") do |cmd|
      if options[:wait] == nil then
        options[:wait] = true;
      end
      options[:master_cmds] ||= []
      if cmd == nil then
        options[:master_cmds] << [:ssh, nil]
      elsif cmd =~ /j-[A-Z0-9]+/ then
        options[:job_flow_ids] ||= []
        options[:job_flow_ids] << cmd
        options[:master_cmds] << [:ssh, nil]
      else
        options[:master_cmds] << [:ssh, cmd]
      end
    end

    set_opt_flag.call( "--logs", "Display the step logs for the last executed step", :step_logs)

    opts.on("--scp SRC", "Copy a file to the master node") do |src|
      if (options[:wait] == nil) then
        options[:wait] = true;
      end
      options[:master_cmds] ||= []
      options[:master_cmds] << [:scp, src]
    end

    opts.on("--to DEST", "the destination to scp a file to") do |dest|
      if options[:master_cmds] == nil then
        raise "--to option must follow an --scp option, it specifies the destination location"
      end
      last_scp = options[:master_cmds].select { |c| c[0] == :scp }.last
      if last_scp == nil then
        raise "--to option must follow an --scp option, it specifies the destination location"
      end
      if last_scp.size > 2 then
        raise "multiple scp destinations are not allowed, --to has been specified twice after an --scp option"
      end
      last_scp << dest
    end

    opts.separator "\n  Settings common to all step types\n"

    all_types = ['jar', 'streaming', 'pig-interactive', 'pig-script', 'hive-interactive', 'hive-script', 'install-jobflow-debugging']
    set_step_opt_value.call( "--step-name STEP_NAME", "Set name for the step", :step_name, all_types)
    set_step_opt_value.call( "--step-action STEP_NAME", "Action to take when step finishes. One of CANCEL_AND_WAIT, TERMINATE_JOB_FLOW or CONTINUE", :step_action, all_types)

    opts.on("--arg ARG", "Specify an argument to a bootstrap action, jar, streaming, pig-script or hive-script step") do |arg|
      if last_type_added == :bootstrap_action then
        bootstrap_action = options[:bootstrap_actions].last
        bootstrap_action[:args] ||= []
        bootstrap_action[:args] << arg
      else
        check_step_set.call("--arg", all_types + ["bootstrap-action"])
        step = options[:steps].last
        step[:args] ||= []
        step[:args] << arg
      end
    end

    opts.on("--args ARGS", "Specify a comma seperated list of arguments, e.g --args 1,2,3 would three arguments") do |arg|
      if last_type_added == :bootstrap_action then
        bootstrap_action = options[:bootstrap_actions].last
        bootstrap_action[:args] ||= []
        bootstrap_action[:args] += arg.split(",")
      else
        check_step_set.call("--args", all_types + ["bootstrap-action"])
        step = options[:steps].last
        step[:args] ||= []
        step[:args] += arg.split(",")
      end
    end

    opts.separator "\n  Specifying Bootstrap Actions\n"
    
    opts.on("--bootstrap-action SCRIPT", "Run a bootstrap action script on all instances") do |bootstrap|
      assert_not_matching("--bootstrap-action", bootstrap, /^--/)
      last_type_added = :bootstrap_action
      options[:bootstrap_actions] ||= []
      options[:bootstrap_actions] << { :path => bootstrap }
    end
    
    opts.on("--bootstrap-name NAME", "Set the name of the bootstrap action") do |name|
      assert_not_matching("--bootstrap-name", name, /^--/)
      if ! options[:bootstrap_actions] || ! options[:bootstrap_actions].last then
        raise "--bootstrap-name must follow one of --bootstrap-action"
      end
      options[:bootstrap_actions].last[:name] = name
    end
    
    opts.separator "Note --arg and --args are used to pass arguments to bootstrap actions"
        
    opts.separator "\n  Listing and Describing Job Flows\n"

    set_opt_flag.call("--list", "List all job flows created in the last 2 days", :list)
    set_opt_flag.call("--describe", "Dump a JSON description of the supplied job flows", :describe)
    set_opt_flag.call("--active", "List running, starting or shutting down job flows", :active)
    set_opt_flag.call("--all", "List all job flows in the last 2 months", :all)
    set_opt_flag.call("--nosteps", "Do not list steps when listing jobs", :nosteps)

    opts.on("--state STATE", "List job flows in STATE") do |state|
      assert_not_matching("--state", state, /^--/)
      options[:states] ||= []
      options[:states] << state.upcase
    end

    opts.on("-n MAX_RESULTS", "--max-results MAX_RESULTS", "Maximum number of results to list") do |max_results|
      assert_matching("-n", max_results, /^[0-9]+$/)
      options[:max_results] = max_results.to_i
    end

    opts.separator "\n  Terminating Job Flows\n"

    set_opt_flag.call("--terminate", "Terminate the job flow", :terminate)

    opts.separator "\n  Common Options\n"

    opts.on("-j", "--jobflow JOB_FLOW_ID", "--job-flow-id JOB_FLOW_ID") do |job_flow| 
      argument_error_if_nil(job_flow.match(/^j-[A-Z0-9]+$/),
        "Job flow #{job_flow} is not in the expected format, expected something like j-ABAHAS0019121")
      options[:job_flow_ids] ||= []
      options[:job_flow_ids] << job_flow
    end

    set_opt_value_with_short.call("-c", "--credentials CRED_FILE", "File containing access-id and private-key", :credentials)
    set_opt_value_with_short.call("-a", "--access-id ACCESS-ID", "AWS Access Id", :aws_access_id)
    set_opt_value_with_short.call("-k", "--private-key PRIVATE-KEY", "AWS Private Key", :aws_secret_key)

    opts.on("-v", "--verbose", "Turn on verbose logging of program interaction") do |verbose|
      options[:verbose] = true
    end


    opts.separator "\n  Uncommon Options\n"

    set_opt_flag.call( "--debug", "Print stack traces when exceptions occur", :debug)
    set_opt_value.call("--endpoint ENDPOINT", "Specify the webservice endpoint to talk to", :endpoint)
    set_opt_value.call("--region REGION", "The region to use for the endpoint", :region)
    set_opt_value.call("--apps-path APPS_PATH", "Specify s3:// path to the base of the emr public bucket to use. e.g s3://us-east-1.elasticmapreduce", :apps_path)
    set_opt_value.call("--beta-path BETA_PATH", "Specify s3:// path to the base of the emr public bucket to use for beta apps. e.g s3://beta.elasticmapreduce", :beta_path)
    
    opts.on("--version", "Print a version string") do |version|
      puts "Version: " + AMAZON_ELASTIC_MAP_REDUCE_CLIENT_VERSION
      exit 0
    end

    opts.on_tail("-h", "--help", "Show help message") do
      puts opts
      exit
    end

  end
  opts.parse!

  if options[:aws_access_id] == nil || options[:aws_secret_key] == nil then
    if options[:aws_access_id] == nil && options[:aws_secret_key] == nil then
      parse_credentials(options[:credentials], options)
    else
      argument_error_if_nil(options[:aws_access_id],
        "Missing access_id, you must supply either -a or -c argument, type --help for usage")
      argument_error_if_nil(options[:aws_secret_key],
                            "Missing secret_key, you must supply either -k or -c argument, type --help for usage")
    end
  end

  options[:args] = ARGV.dup
  
  for arg in options[:args] do
    if arg =~ /^j-\w{5,20}$/  then
      options[:job_flow_ids] ||= []
      options[:job_flow_ids] << arg
    end
  end

  # check implications between arguments, note: the conclusions are disjunctive forms
  #   premise -> or (conclusion)
  implications = {
    :alive                => :create,
    :max_results          => :list,
    :active               => [:list, :describe],
    :nosteps              => :list,
    :num_instances        => :create,
    :instance_type        => :create,
    :master_instance_type => :create,
    :slave_instance_type  => :create,
    :steps                => [:create, :job_flow_ids],
    :name                 => :create,
  }

  # negated disjunctive implications of the form: 
  #   premise -> not ( or (conclusion) )
  exclusions = {
    :all       => [:active, :states],
    :list      => [:describe]
  }

  key_set = Set.new(options.keys)
  for premise, conclusion in implications do
    if options[premise] then
      if conclusion.is_a?(Array) then
        if key_set.intersection(Set.new(conclusion)).size == 0 then
          raise ArgumentError, "Option #{premise.to_s} requires at least one of #{conclusion.join(", ")}."
        end
      else
        if ! key_set.member?(conclusion) then
          raise ArgumentError, "Option #{premise.to_s} requires the option #{conclusion}"
        end
      end
    end
  end

  for premise, conclusion in exclusions do
    if options[premise] then
      if conclusion.is_a?(Array) then
        if key_set.intersection(Set.new(conclusion)).size != 0 then
          raise ArgumentError, "Option #{premise.to_s} may not be combined with #{conclusion.join(", ")}"
        end
      else
        if key_set.member?(conclusion) then
          raise ArgumentError, "Option #{premise.to_s} may not be combined with #{conclusion}"
        end
      end
    end
  end

  if options[:create] && options[:enable_debugging] && options[:log_uri] == nil then
    raise ArgumentError, "If you enable debugging then you must specify a log_uri"
  end

  if options[:job_flow_ids] && options[:create] then
    raise ArgumentError, "You cannot specify both --create and --job-flow-ids"
  end

rescue SystemExit => e
  exit -1
rescue Exception => e
  puts "Error: #{e.message}"
  trace_exception(options, e)
  exit -1
end

if options[:region] then
  if options[:endpoint] then
    raise ArgumentError, "You cannot specify both --endpoint and --region"
  end
  
  options[:endpoint] = "https://#{options[:region]}.elasticmapreduce.amazonaws.com"
end

if options[:endpoint] && !options[:apps_path] then
  region = options[:endpoint].match("^https*://(.*)\.elasticmapreduce") 
  if region then
    options[:apps_path] = "s3://#{region[1]}.elasticmapreduce"
  end
end
options[:apps_path] ||= "s3://us-east-1.elasticmapreduce"
options[:apps_path].chomp!("/")
options[:beta_path] ||= "s3://beta.elasticmapreduce"
options[:beta_path].chomp!("/")

# Example showing how to initialize and call the client
config = {
  :endpoint            => options[:endpoint] || "https://elasticmapreduce.amazonaws.com",
  :ca_file             => File.join(File.dirname(__FILE__), "cacert.pem"),
  :aws_access_key      => options[:aws_access_id],
  :aws_secret_key      => options[:aws_secret_key],
  :signature_algorithm => :V2,
  :verbose             => (options[:verbose] != nil)
}

client = Amazon::Coral::ElasticMapReduceClient.new_aws_query(config)

# wrap the client in a retry Delegator that will retry on error responses
client = Amazon::RetryDelegator.new(client, :retry_if => is_retryable_error_response)

def format(map, *fields)
  result = []
  for field in fields do
    key = field[0].split(".")
    value = map
    while key.size > 0 do
      value = value[key.first]
      key.shift
    end
    result << sprintf("%-#{field[1]}s", value)
  end
  result.join("")
end

def extra_args(args)
  if args && args.size > 0 then
    [ "--args" ] + args
  else
    return []
  end
end

begin
  script_runner_path     = File.join(options[:apps_path], "libs/script-runner/script-runner.jar")
  pig_path               = File.join(options[:apps_path], "libs/pig/")
  hive_path              = File.join(options[:apps_path], "libs/hive/")
  pig_cmd                = [ File.join(pig_path, "pig-script"), "--base-path", pig_path ]
  hive_cmd               = [ File.join(hive_path, "hive-script"), "--base-path", hive_path ]
  
  enable_debugging_path  = File.join(options[:apps_path], "libs/state-pusher/0.1")
  
  step_prototypes = {
    "install-jobflow-debugging" => Proc.new do |step_options| 
      step = {
        "Name"            => step_options[:step_name] || "Setup Hadoop Debugging",
        "ActionOnFailure" => step_options[:step_action] || "TERMINATE_JOB_FLOW",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => [ File.join(enable_debugging_path, "fetch") ] + (step_options[:args] || [])
        }
      }
      [ step ]
    end,

    "pig-interactive" => Proc.new do |step_options| 
      step = {
        "Name"            => step_options[:step_name] || "Setup Pig",
        "ActionOnFailure" => step_options[:step_action] || "TERMINATE_JOB_FLOW",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => pig_cmd + [ "--install-pig", extra_args(step_options[:args]) ]
        }
      }
      [ step ]
    end,

    "pig-script" => Proc.new do |step_options| 
      if !step_options[:args] || step_options[:args].size == 0 then
        raise ArgumentError, "When using --pig-script, you must pass the script location in S3 using a --arg parameter."
      end
      
      step = {
        "Name"            => step_options[:step_name] || "Run Pig Script",
        "ActionOnFailure" => step_options[:step_action] || "CANCEL_AND_WAIT",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => pig_cmd + [ "--run-pig-script", "--args", "-f" ] + (step_options[:args] || [])
        }
      }
      [ step ]
    end,
    
    "hive-site" => Proc.new do |step_options| 
      step = {
        "Name"            => step_options[:step_name] || "Install Hive Site Configuration",
        "ActionOnFailure" => step_options[:step_action] || "CANCEL_AND_WAIT",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => hive_cmd + [ "--install-hive-site", "--hive-site=#{step_options[:site]}" ] + 
            (step_options[:args] || [])
        }
      }
      [ step ]
    end,

    "hive-interactive" => Proc.new do |step_options| 
      step = {
        "Name"            => step_options[:step_name] || "Setup Hive",
        "ActionOnFailure" => step_options[:step_action] || "TERMINATE_JOB_FLOW",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => hive_cmd + [ "--install-hive", extra_args(step_options[:args]) ]
        }
      }
      [ step ]
    end,

    "hive-script" => Proc.new do |step_options| 
      if !step_options[:args] || step_options[:args].size == 0 then
        raise ArgumentError, "When using --hive-script, you must pass the script location in S3 using a --arg parameter."
      end
      
      step = {
        "Name"            => step_options[:step_name] || "Run Hive Script",
        "ActionOnFailure" => step_options[:step_action] || "CANCEL_AND_WAIT",
        "HadoopJarStep"   => {
          "Jar" => script_runner_path,
          "Args" => hive_cmd + [ "--run-hive-script", "--args", "-f" ] + (step_options[:args] || [])
        }
      }
      [ step ]
    end,
    
    "streaming" => Proc.new do |step_options| 
      if step_options[:mapper] == nil then
        step_options[:mapper] = "s3n://elasticmapreduce/samples/wordcount/wordSplitter.py"
      end
      timestr = Time.now.strftime("%Y-%m-%dT%H%M%S")
      stream_options = []
      if step_options[:cache] then
        for ca in step_options[:cache] do
          stream_options << "-cacheFile" << ca
        end
      end
      if step_options[:cache_archive] then
        for ca in step_options[:cache_archive] do
          stream_options << "-cacheArchive" << ca
        end
      end
      if step_options[:jobconf] then
        for jc in step_options[:jobconf] do
          stream_options << "-jobconf" << jc
        end
      end
      # Note that the streaming options should go before command options for
      # Hadoop 0.20
      step = {
        "Name"            => step_options[:step_name] || "Example Streaming Step",
        "ActionOnFailure" => step_options[:step_action] || "CANCEL_AND_WAIT",
        "HadoopJarStep"   => {
          "Jar" => "/home/hadoop/contrib/streaming/hadoop-streaming.jar",
          "Args" => (sort_streaming_args(step_options[:args]) || []) + (stream_options) + [
            "-input",     step_options[:input]   || "s3n://elasticmapreduce/samples/wordcount/input",
            "-output",    step_options[:output]  || "hdfs:///examples/output/#{timestr}",
            "-mapper",    step_options[:mapper],
            "-reducer",   step_options[:reducer] || "aggregate"
          ]
        }
      }
      [ step ]
    end,

    "jar" => Proc.new do |step_options| 
      step = {
        "Name"            => step_options[:step_name] || "Example Jar Step",
        "ActionOnFailure" => step_options[:step_action] || "CANCEL_AND_WAIT",
        "HadoopJarStep"   => {
          "Jar"  => step_options[:jar],
          "Args" => step_options[:args] || []
        }
      }
      step["HadoopJarStep"]["MainClass"] = step_options[:main_class] if step_options[:main_class]
      [ step ]
    end,

    "json" => Proc.new do |step_options|
      content = steps = nil
      begin
        content = File.read(step_options[:file])
      rescue Exception => e
        raise RuntimeError, "Couldn't read json file #{step_options[:file]}"
      end
      if step_options[:variables] then
        for var, value in step_options[:variables] do
          content.gsub!(var, value)
        end
      end
      begin
        steps = JSON.parse(content)
      rescue Exception => e
        raise RuntimeError, "Error parsing json from file #{step_options[:file]}"
      end
      if steps.is_a?(Array) then
        steps
      else
        [ steps ]
      end
    end
  }

  # look to see if the jobflow has already had pig or hive installed
  setup_pig = false
  setup_hive = false
  if ! options[:create] && options[:steps] then
    if options[:job_flow_ids].size == 1 then
      jobflow = check_for_error("Error listing job #{options[:job_flow_ids].join(", ")}: ") do
        client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
      end
      jobflow_version = jobflow / 'HadoopVersion'
      jobflow_steps = jobflow / 'JobFlows' / 0 / 'Steps'
      for jobflow_step in jobflow_steps do
        step_config = jobflow_step / 'StepConfig' / 'HadoopJarStep' 
        jar = step_config / 'Jar'
        argument = step_config / 'Args' / 3
        if jar == script_runner_path then
          if argument == "--install-hive" then  
            setup_hive = true
          end
        end
        if jar == script_runner_path then
          if argument == "--install-pig" then
            setup_pig = true
          end
        end
      end
    end
  end
  
  # apply the step prototypes
  steps = []

  # if the state pusher is being installed move it to the head of the queue
  enable_debugging_steps = if options[:steps] then
                   options[:steps].select { |step| step[:type] == 'install-jobflow-debugging' }
                 else
                   []
                 end
  if options[:enable_debugging] || enable_debugging_steps.size > 0 then
    if enable_debugging_steps.size > 1 then
      puts "More than one enable_debugging step specified, install debugging will be run once as the first step"
    end
    options[:steps] = options[:steps].select { |step| step[:type] != 'install-jobflow-debugging' }
    if enable_debugging_steps.size > 0 then
      step  =  enable_debugging_steps.first
      steps += step_prototypes[step[:type]].call(step)
    else
      steps += step_prototypes["install-jobflow-debugging"].call({})
    end
  end

  # check for hive and pig steps to ensure they are preceeded by install steps
  if options[:steps] then
    for step in options[:steps] do
      proto = step_prototypes[step[:type]]
      if ! proto then
        raise ArgumentError, "There is no step type called #{step_name}"
      else
        if step[:type] == "pig-script" && !setup_pig then
          setup_pig = true
          steps += step_prototypes["pig-interactive"].call({})
        elsif step[:type] == "hive-script" && !setup_hive then
          setup_hive = true
          steps += step_prototypes["hive-interactive"].call({})
        elsif step[:type] == "hive-site" && !setup_hive then
          setup_hive = true
          steps += step_prototypes["hive-interactive"].call({})
        elsif step[:type] == "pig-interactive" then
          if options[:create] && !options[:key_pair] then
            raise ArgumentError, "You need to set the --key-pair argument to do an interactive (SSH) Pig session."
          end
          if options[:create] && !options[:alive] then
            raise ArgumentError, "You need to set the --alive argument to do an interactive (SSH) Pig session. The job flow will remain alive for your session and you then must terminate it."
          end
          setup_pig = true
        elsif step[:type] == "hive-interactive" then
          if options[:create] && !options[:key_pair] then
            raise ArgumentError, "You need to set the --key-pair argument to do an interactive (SSH) Hive session."
          end
          if options[:create] && !options[:alive] then
            raise ArgumentError, "You need to set the --alive argument to do an interactive (SSH) Hive session. The job flow will remain alive for your session and you then must terminate it."
          end
          setup_hive = true
        end
        
        steps += proto.call(step)
      end
    end
  end

  default_job_flow_name = "Development Job Flow "
  if options[:alive] then
    default_job_flow_name += " (requires manual termination)"
  end
  job_flow = {
    "Name"   => options[:name] || default_job_flow_name,
    "Instances" => {
      "SlaveInstanceType"           => options[:slave_instance_type] || options[:instance_type] || "m1.small",
      "MasterInstanceType"          => options[:master_instance_type] || options[:instance_type] || "m1.small",
      "InstanceCount"               => options[:num_instances] || "1",
      "KeepJobFlowAliveWhenNoSteps" => (options[:alive] ? "true" : "false")
    },
    "Steps" => steps
  }

  if options[:ainfo] then
    job_flow["AdditionalInfo"] = options[:ainfo]
  end

  if options[:key_pair] then
    job_flow["Instances"]["Ec2KeyName"] = options[:key_pair]
  end
  
  if options[:hadoop_version] then
    job_flow["Instances"]["HadoopVersion"] = options[:hadoop_version]
  else
    if options[:create] then
      job_flow["Instances"]["HadoopVersion"] = DEFAULT_HADOOP_VERSION 
    end
  end

  if options[:az] then
    job_flow["Instances"]["Placement"] ||= {}
    job_flow["Instances"]["Placement"]["AvailabilityZone"] = options[:az]
  end

  if options[:log_uri] then
    job_flow["LogUri"] = options[:log_uri]
  end

  if options[:bootstrap_actions] then
    job_flow["BootstrapActions"] = []
      
    i = 0;
    for bootstrap_action in options[:bootstrap_actions] do
      action = { "Name" => bootstrap_action[:name] || "Bootstrap #{i}", "ScriptBootstrapAction" => {}}
      action["ScriptBootstrapAction"]["Path"] = bootstrap_action[:path]
      if bootstrap_action[:args] then
        action["ScriptBootstrapAction"]["Args"] = bootstrap_action[:args]
      end
      
      job_flow["BootstrapActions"] << action
      i += 1
    end
  end
  
  if options[:debug] then
    puts "jobflow=" + job_flow.inspect
    puts "steps=" + steps.inspect
  end

  if options[:create] then
    result = client.RunJobFlow(job_flow)
    if result && result.key?('JobFlowId') then
      job_flow_id = result['JobFlowId']
      options[:job_flow_ids] ||= []
      options[:job_flow_ids] << job_flow_id
      if options[:plain_output] then
        puts job_flow_id
      else
        puts "Created job flow " + job_flow_id
      end
    else
      raise RuntimeError, "creating job flow: " + JSON.pretty_generate(result)
    end
  end

  if options[:steps] && ! options[:create] then
    if options[:job_flow_ids].size != 1 then
      raise RuntimeError, "To call add steps you must specify either --create or exactly one job_flow_id"
    end
    job_flow_id = options[:job_flow_ids].first
    result = client.AddJobFlowSteps('JobFlowId' => job_flow_id, 'Steps' => steps)
    if result != nil then
      raise RuntimeError, "Error adding steps: " + JSON.pretty_generate(result)
    else
      puts "Added steps to #{job_flow_id}"
      if options[:wait_for_step] then
        puts "Waiting for the step to finish"
        begin
          describe_result = client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
          ret_jobflows = describe_result['JobFlows']
          if ret_jobflows != nil then
            retjob = ret_jobflows.first
          else
            next  
          end
          sleep(30)
        end until retjob['ExecutionStatusDetail']['State'] == "WAITING" || retjob['ExecutionStatusDetail']['State'] == "FAILED"
      end
    end
  end

  result = nil
  if options[:list] || options[:describe] then
    if options[:job_flow_ids] && options[:job_flow_ids].size > 0 then
      result = check_for_error("Error listing job #{options[:job_flow_ids].join(", ")}: ") do
        client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
      end
    else
      states = []
      if options[:active] then
        states = %w(RUNNING SHUTTING_DOWN STARTING WAITING BOOTSTRAPPING)
      end
      if options[:states] then
        states += options[:states]
      end
      if options[:active] || options[:states] then
        result = client.DescribeJobFlows('JobFlowStates' => states)
      elsif options[:all] then
        result = client.DescribeJobFlows()
      else
        result = client.DescribeJobFlows('CreatedAfter' => (Time.now - (24 * 3600)).xmlschema)
      end
      if result.key?("JobFlows") then
        options[:job_flow_ids] ||= []
        options[:job_flow_ids] += result['JobFlows'].map { |x| x['JobFlowId'] }
        options[:job_flow_ids].uniq!
      else
        raise RuntimeError, "Error listing jobs: " + JSON.pretty_generate(result)
      end
    end

    if options[:list] then
      job_flows = result['JobFlows']
      count = 0
      for job_flow in job_flows do 
        if options[:max_results] && (count += 1) > options[:max_results] then
          break
        end
        puts format(job_flow, ['JobFlowId', 20], ['ExecutionStatusDetail.State', 15], 
                    ['Instances.MasterPublicDnsName', 50]) + job_flow['Name']
        if ! options[:nosteps] then
          for step in job_flow['Steps'] do
            puts "   " + format(step, ['ExecutionStatusDetail.State', 15], ['StepConfig.Name', 30])
          end
        end
      end
    elsif options[:describe] then
      puts JSON.pretty_generate(result)
    end
  end

  if options[:terminate] then
    if options[:job_flow_ids] == nil || options[:job_flow_ids].size == 0 then
      raise RuntimeError, "You must specify job flow ids of job flows you wish to terminate"
    end
    result = client.TerminateJobFlows('JobFlowIds' => options[:job_flow_ids])
    if result != nil then
      raise RuntimeError, "Error terminating job: " + JSON.pretty_generate(result)
    else
      puts "Job #{options[:job_flow_ids].join(", ")} Terminated"
    end
  end

  if options[:debug] then
    puts "Keypair=" + options[:key_pair].inspect
  end

  if options[:step_logs] || options[:master_cmds] then
    if options[:job_flow_ids] == nil || options[:job_flow_ids].size == 0 then
      raise RuntimeError, "You must specify a job flow id to show the logs or ssh to the master"
    end

    result = check_for_error("Error listing job #{options[:job_flow_ids].join(", ")}: ") do
      client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
    end
    
    job_flows = result['JobFlows']
    if options[:step_logs] then
      for job_flow in job_flows do 
        hostname = job_flow['Instances']['MasterPublicDnsName']
        step_id = job_flow['Steps'].size
        key_pair_file = options[:key_pair_file]
        if key_pair_file == nil || key_pair_file.size == 0 then
          raise RuntimeError, "You must supply a key-pair-file to run this command"
        end
        cmd = "ssh -i #{key_pair_file} hadoop@#{hostname} cat /mnt/var/log/hadoop/steps/#{step_id}/{syslog,stderr,stdout}"
        puts cmd
        system cmd
      end
    end

    if options[:master_cmds] then
      if job_flows.size != 1 then
        raise RuntimeError, "Expected a single jobflow when performing ssh or scp"
      end
      job_flow = job_flows.first
      hostname = job_flow['Instances']['MasterPublicDnsName']
      key_pair_file = options[:key_pair_file]
      if key_pair_file == nil || key_pair_file.size == 0 then
        raise RuntimeError, "You must supply a key-pair-file to run this command"
      end

      if options[:wait] == true then
        describe_result = client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
        ret_jobflows = describe_result['JobFlows']
        ret_job = ret_jobflows.first

        user_notified_of_wait = false
        while ! is_closed_down(ret_job) && ! is_waiting_or_running(ret_job) do
          if ! user_notified_of_wait then
            puts "Waiting for job flow to enter running or waiting state"
            user_notified_of_wait = true
          end
          sleep(30)
          describe_result = client.DescribeJobFlows('JobFlowIds' => options[:job_flow_ids])
          ret_jobflows = describe_result['JobFlows']
          ret_job = ret_jobflows.first
        end

        if is_closed_down(ret_job) then
          raise RuntimeError, "Job flow entered state #{jobflow_state(ret_job)} while waiting for job flow to start"
        end

        hostname = ret_job['Instances']['MasterPublicDnsName']
      end
        
      for master_cmd in options[:master_cmds] do
        if master_cmd[0] == :scp then
          src = master_cmd[1]
          dest = master_cmd[2] || ""
          cmd = "scp -o StrictHostKeyChecking=no -i #{key_pair_file} -r '#{src}' hadoop@#{hostname}:'#{dest}'"
          puts cmd
          system cmd
        elsif master_cmd[0] == :ssh then
          cmd = "ssh -o StrictHostKeyChecking=no -i #{key_pair_file} hadoop@#{hostname}"
          cmd_string = master_cmd[1]
          if cmd_string then
            cmd += " '" + cmd_string + "'"
          end
          puts cmd
          system cmd
        else
          puts "No match for #{master_cmd.inspect}"
        end
      end
    end
  end
 
  standard_options = [:terminate, :debug, :list, :steps, :create, :describe, :step_logs, :master_cmds]
  if Set.new(options.keys).intersection(Set.new(standard_options)).size == 0 then
    puts opts
    exit 0
  end

rescue SystemExit => e
  exit -1
rescue Exception => e
  puts "Error: #{e.message}"
  trace_exception(options, e)
  exit -1
end
